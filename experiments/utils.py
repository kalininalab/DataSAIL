import os
import re
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional, Union, Any, Tuple

import deepchem as dc
import esm
import numpy as np
import pandas as pd
import torch
from chemprop.train import prc_auc
from matplotlib import pyplot as plt
from matplotlib.lines import Line2D
from rdkit import Chem
from rdkit.Chem import AllChem
import matplotlib.transforms as mtransforms
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, \
    GradientBoostingClassifier
from sklearn.manifold import TSNE
from sklearn.metrics import mean_absolute_error, roc_auc_score, mean_squared_error
from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.svm import LinearSVR, LinearSVC

from datasail.settings import TEC_I1, MODE_E, MODE_F, TEC_I2, TEC_C1, TEC_C2

MPP_EPOCHS = 50
RUNS = 5
USE_UMAP = False  # if False uses tSNE
HSPACE = 0.25

DATASETS = {
    "qm7": [dc.molnet.load_qm7, "regression", "mae", 7160],
    "qm8": [dc.molnet.load_qm8, "regression", "mae", 21786],
    "qm9": [dc.molnet.load_qm9, "regression", "mae", 133885],
    "esol": [dc.molnet.load_delaney, "regression", "rmse", 1128],
    "freesolv": [dc.molnet.load_freesolv, "regression", "rmse", 642],
    "lipophilicity": [dc.molnet.load_lipo, "regression", "rmse", 4200],
    "pcba": [dc.molnet.load_pcba, "classification", "prc-auc", 327929],
    "muv": [dc.molnet.load_muv, "classification", "prc-auc", 93087],
    "hiv": [dc.molnet.load_hiv, "classification", "auc", 41127],
    "bace": [dc.molnet.load_bace_classification, "classification", "auc", 1513],
    "bbbp": [dc.molnet.load_bbbp, "classification", "auc", 2039],
    "tox21": [dc.molnet.load_tox21, "classification", "auc", 7831],
    "toxcast": [dc.molnet.load_toxcast, "classification", "auc", 8575],
    "sider": [dc.molnet.load_sider, "classification", "auc", 1427],
    "clintox": [dc.molnet.load_clintox, "classification", "auc", 1478],
}
METRICS = {"mae": "MAE ↓", "rmse": "RMSE ↓", "prc-auc": "PRC-AUC ↑", "auc": "ROC-AUC ↑"}

models = {
    "rf-r": RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=42),
    "svm-r": MultiOutputRegressor(LinearSVR(random_state=42, dual="auto")),
    "xgb-r": MultiOutputRegressor(GradientBoostingRegressor(random_state=42)),
    "mlp-r": MLPRegressor(hidden_layer_sizes=(512, 256, 64), random_state=42, max_iter=4 * MPP_EPOCHS),
    "rf-c": RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42),
    "svm-c": MultiOutputClassifier(LinearSVC(random_state=42, dual="auto")),
    "xgb-c": MultiOutputClassifier(GradientBoostingClassifier(random_state=42)),
    "mlp-c": MLPClassifier(hidden_layer_sizes=(512, 256, 64), random_state=42, max_iter=4 * MPP_EPOCHS),
}
metric = {
    "mae": mean_absolute_error,
    "rmse": lambda pred, truth: mean_squared_error(pred, truth, squared=False),
    "prc-auc": prc_auc,
    "auc": roc_auc_score,
}
COLORS = {
    "test": "#5D3A9B",
    "train": "#E66100",
    "0d": "#994F00",
    "i2": "#DC3220",
    "c2": "#1AFF1A",

    "r1d": "#0C7BDC",
    "i1e": "#0C7BDC",  # ?
    "s1d": "#FFC20A",
    "c1e": "#FFC20A",  # ?

    "lohi": "#E66100",
    "graphpart": "#5D3A9B",
    "butina": "#994F00",
    "fingerprint": "#0C7BDC",
    "maxmin": "#DC3220",
    "scaffold": "#FFC20A",
    "weight": "#1AFF1A",
    "drop": "#808080",
}
SPLITTERS = {
    "Scaffold": dc.splits.ScaffoldSplitter(),
    "Weight": dc.splits.MolecularWeightSplitter(),
    "MaxMin": dc.splits.MaxMinSplitter(),
    "Butina": dc.splits.ButinaSplitter(),
    "Fingerprint": dc.splits.FingerprintSplitter(),
}
TECHNIQUES = {
    "datasail": ["R", TEC_I1 + MODE_E, TEC_I1 + MODE_F, TEC_I2, TEC_C1 + MODE_E, TEC_C1 + MODE_F, TEC_C2],
    "deepchem": ["Butina", "Fingerprint", "MaxMin", "Scaffold", "Weight"],
    "lohi": ["lohi"],
    "graphpart": ["graphpart"]
}
DRUG_TECHNIQUES = [TEC_I1 + MODE_E, TEC_C1 + MODE_E, "lohi"] + TECHNIQUES["deepchem"]
PROTEIN_TECHNIQUES = [TEC_I1 + MODE_F, TEC_C1 + MODE_F, "graphpart"]


def save_datasail_splits(base: Path, df: pd.DataFrame, key: str, techniques: List[Tuple[str, str]], e_splits: Optional[Dict] = None,
                         inter_splits: Optional[Dict] = None) -> None:
    """
    Save the splits generated by DataSAIL.

    Args:
        base: Path to the base directory
        df: DataFrame to split
        key: Key to use for the splits
        techniques: List of techniques to use
        e_splits: Splits along dimension e
        inter_splits: Interactions splits
    """
    for name, tech in techniques:
        # for run in range(RUNS):
        for run in range(1):
            path = base / name / f"split_{run}"
            path.mkdir(parents=True, exist_ok=True)

            for label in ["train", "test"]:
                if inter_splits is not None:
                    sub = df[key].apply(lambda x: inter_splits[tech][run].get((x, x), "") == label)
                else:
                    sub = df[key].apply(lambda x: e_splits[tech][run].get(x, "") == label)
                df[sub].to_csv(path / f"{label}.csv", index=False)


class ESM2T12:
    _instance = None
    _model, _alphabet, _batch_converter = None, None, None

    @classmethod
    def instance(cls):
        if cls._instance is None:
            cls._instance = cls.__new__(cls)
            cls._model, cls._alphabet = esm.pretrained.esm2_t12_85M_UR50S()
            cls._batch_converter = cls._alphabet.get_batch_converter()
            cls._model.eval()
        return cls._instance

    def embed_aaseqs(self, aaseq: str) -> List[float]:
        """
        Embed a protein sequence using an ESM model.

        Args:
            aaseq: Amino acid sequence to embed

        Returns:
            np.ndarray: Array of floats representing the sequence
        """
        batch_labels, batch_strs, batch_tokens = self._batch_converter([("query", aaseq)])
        batch_lens = (batch_tokens != self._alphabet.padding_idx).sum(1)
        with torch.no_grad():
            results = self._model(batch_tokens, repr_layers=[12], return_contacts=True)
            token_representations = results["representations"][12]

            sequence_representations = []
            for i, tokens_len in enumerate(batch_lens):
                sequence_representations.append(token_representations[i, 1: tokens_len - 1].mean(0))
            return sequence_representations[0].numpy()


def embed_sequence(aa_seq: str, prot_embeds: Dict[str, Optional[np.ndarray]] = None) -> Optional[np.ndarray]:
    """
    Embed a protein sequence using an ESM model.

    Args:
        aa_seq: Amino acid sequence to embed
        prot_embeds: Dictionary of already embedded sequences

    Returns:
        List[float]: List of floats representing the sequence
    """
    if prot_embeds is None:
        prot_embeds = {}
    amino_acids_pattern = re.compile('[^ACDEFGHIKLMNPQRSTVWY]')
    aa_seq = amino_acids_pattern.sub('G', aa_seq)[:1022]
    if aa_seq not in prot_embeds:
        try:
            prot_embeds[aa_seq] = ESM2T12().instance().embed_aaseqs(aa_seq)
        except:
            prot_embeds[aa_seq] = None
    return prot_embeds[aa_seq]


def embed_smiles(
        smile: str, drug_embeds: Dict[Union[str, float], Optional[List[float]]], n_bits: int = 480
) -> Optional[np.ndarray]:
    """
    Embed a SMILES string using the Morgan fingerprint.

    Args:
        smile: SMILES string to embed
        drug_embeds: Dictionary of already embedded drugs
        n_bits: Number of bits to use for the fingerprint

    Returns:
        Optional[List[float]]: List of floats representing the fingerprint
    """
    try:
        if smile not in drug_embeds:
            # Check for invalid strings
            mol = smiles2mol(smile)
            if mol is None:
                drug_embeds[smile] = None

            # Compute a new Fingerprint
            drug_embeds[smile] = np.array(list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)))
    except:
        drug_embeds[smile] = None
    return drug_embeds[smile]


def smiles2mol(smiles: Any) -> Optional[Chem.Mol]:
    """
    Convert a SMILES string to a RDKit molecule.

    Args:
        smiles: SMILES string to convert

    Returns:
        Optional[Chem.Mol]: RDKit molecule
    """
    if smiles != smiles or not isinstance(smiles, str) or len(smiles) == 0:
        return None

    return Chem.MolFromSmiles(smiles)


def mol2smiles(mol: Chem.Mol) -> Optional[str]:
    """
    Convert a RDKit molecule to a SMILES string.

    Args:
        mol: RDKit molecule

    Returns:
        Optional[str]: SMILES string
    """
    try:
        return Chem.MolToSmiles(Chem.rdmolops.RemoveHs(mol))
    except:
        return None


def load_lp_pdbbind() -> pd.DataFrame:
    """
    Load the LP_PDBBind dataset.

    Returns:
        pd.DataFrame: The dataset
    """
    df = pd.read_csv(Path("experiments") / "DTI" / "lppdbbind" / "dataset" / "LP_PDBBind.csv", index_col=0)
    df["ids"] = df.index
    df = df[["ids", "smiles", "seq", "value"]]
    df.dropna(inplace=True)
    df = df[df.apply(lambda x: len(x["smiles"]) <= 200 and len(x["seq"]) <= 2000, axis=1)]
    df = df[df["smiles"].apply(lambda x: smiles2mol(x) is not None)]
    return df


def dc2pd(ds, ds_name):
    """
    Convert a DeepChem dataset to a pandas DataFrame.

    Args:
        ds: DeepChem dataset
        ds_name: Name of the dataset

    Returns:
        pd.DataFrame: The dataset as a DataFrame
    """
    df = ds.to_dataframe()
    name_map = dict([(f"y{i + 1}", task) for i, task in enumerate(ds.tasks)] + [("y", ds.tasks[0]), ("X", "SMILES")])
    df.rename(columns=name_map, inplace=True)
    df["ID"] = [f"Comp{i + 1:06d}" for i in range(len(df))]
    if ds_name not in ["qm7", "qm8", "qm9"]:
        df["SMILES"] = df["SMILES"].apply(smiles2mol)
        df = df[df["SMILES"].notna()]
    df["SMILES"] = df["SMILES"].apply(mol2smiles)
    if DATASETS[ds_name][1][0] == "classification":
        df[ds.tasks.tolist()] = pd.to_numeric(df[ds.tasks.tolist()], downcast="integer")
        return df[["ID", "SMILES", "w"] + ds.tasks.tolist()]
    return df[["ID", "SMILES"] + ds.tasks.tolist()]


def set_subplot_label(ax: plt.Axes, fig: plt.Figure, label: str) -> None:
    """
    Set the label for a subplot.
    Args:
        ax: The subplot
        fig: The figure
        label: The label to set
    """
    ax.text(
        0.0,
        1.0,
        label,
        transform=ax.transAxes + mtransforms.ScaledTranslation(
            -25 / 72,
            10 / 72,
            fig.dpi_scale_trans
        ),
        fontsize="x-large",
        va="bottom",
        fontfamily="serif",
    )


def embed(full_path, name: Optional[str] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Embed the MoleculeNet dataset using t-SNE.

    Args:
        full_path: Path to the base directory
        name: Name of the dataset

    Returns:
        Tuple: np.arrays containing embedded data from training and test of I1 and S1 splits
    """
    print("Embedding - read data ...")
    if name is None:
        i_tr = pd.read_csv(full_path / "deepchem" / "split_0" / "train.csv")
        i_te = pd.read_csv(full_path / "deepchem" / "split_0" / "test.csv")
        c_tr = pd.read_csv(full_path / "datasail" / "d_0.2_e_0.2" / "split_0" / "train.csv")
        c_te = pd.read_csv(full_path / "datasail" / "d_0.2_e_0.2" / "split_0" / "test.csv")
    else:
        i_tr = pd.read_csv(full_path / "datasail" / name / "I1e" / "split_0" / "train.csv")
        i_te = pd.read_csv(full_path / "datasail" / name / "I1e" / "split_0" / "test.csv")
        c_tr = pd.read_csv(full_path / "datasail" / name / "C1e" / "split_0" / "train.csv")
        c_te = pd.read_csv(full_path / "datasail" / name / "C1e" / "split_0" / "test.csv")

    print("Embedding - compute fingerprints ...")
    smiles = [(s, embed_smiles(s, {})) for s in set(list(i_tr["SMILES"]) + list(i_te["SMILES"]) +
                                                    list(c_tr["SMILES"]) + list(c_te["SMILES"]))]
    ids, fps = zip(*smiles)

    print("Embedding - compute t-SNE ...")
    embedder = TSNE(n_components=2, learning_rate="auto", init="random", random_state=42)
    embeddings = embedder.fit_transform(np.array(fps))

    print("Embedding - relocate samples ...")
    embed_map = {idx: emb for idx, emb in zip(ids, embeddings)}
    return np.stack(i_tr["SMILES"].apply(lambda x: embed_map[x])), \
        np.stack(i_te["SMILES"].apply(lambda x: embed_map[x])), \
        np.stack(c_tr["SMILES"].apply(lambda x: embed_map[x])), \
        np.stack(c_te["SMILES"].apply(lambda x: embed_map[x]))


def plot_embeds(ax: plt.Axes, train, test, title, legend: Optional[Union[str, int]] = None) -> None:
    """
    Plot the t-SNE embeddings of the datasets.

    Args:
        ax: The axis to plot on
        train: Training data
        test: Test data
        title: Title of the plot
        legend: Location of the legend
    """
    n_train = len(train)
    n_test = len(test)

    p = np.concatenate([train, test])
    c = np.array([COLORS["train"]] * n_train + [COLORS["test"]] * n_test)
    perm = np.random.permutation(len(p))
    ax.scatter(p[perm, 0], p[perm, 1], s=5, c=c[perm])
    ax.tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)
    ax.set_title(title)
    ax.set_xlabel("t-SNE 1")
    ax.set_ylabel("t-SNE 2")
    if legend:
        handles, labels = ax.get_legend_handles_labels()
        train_dot = Line2D([0], [0], marker='o', label="train", color=COLORS["train"], linestyle='None')
        test_dot = Line2D([0], [0], marker='o', label="test", color=COLORS["test"], linestyle='None')
        handles.extend([train_dot, test_dot])
        ax.legend(handles=handles, loc="lower right", markerscale=2)
